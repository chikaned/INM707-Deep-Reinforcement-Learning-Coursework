{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb646772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DSProjects\\Anaconda\\envs\\torch_gym\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transform\n",
    "import gym\n",
    "import warnings\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b80bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import DQN\n",
    "from memory import ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48339a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LL:\n",
    "    def __init__(self, state_size, action_size, seed, batch_size=64, gamma=0.99, learning_rate=1e-4,\n",
    "                 buffer_size=int(1e5), n_every=4, tau=1e-3, device = DEVICE, noisy = False, pr_xp = False):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(0)\n",
    "        self.batch_size = 64\n",
    "        self.buffer_size = int(1e5)\n",
    "        self.n_update = 4\n",
    "        self.Loss = 0\n",
    "        \n",
    "        #hyperparameters\n",
    "        self.tau = tau\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.policy_net = DQN(self.state_size, self.action_size).to(DEVICE)\n",
    "        self.target_net = DQN(self.state_size, self.action_size).to(DEVICE)\n",
    "        self.memory = ReplayMemory(self.action_size, self.buffer_size, self.batch_size, self.seed)\n",
    "\n",
    "        #get dict for policy\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval() #set to evalution model\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)#use adam adaptive gradient descent optimizer\n",
    "        self.n_step = 0  # initialize timestep var\n",
    "\n",
    "    def step(self, state, action, reward, next_state, terminal):\n",
    "        self.memory.add_memory(state, action, reward, next_state, terminal) #add to memory\n",
    "        self.n_step = (self.n_step + 1) % self.n_update #update per time steps\n",
    "        if self.n_step == 0: #get experience\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.train_model(experiences, self.gamma)\n",
    "\n",
    "    def decide_action(self, state, epsilon=0):\n",
    "        return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def soft_update(self, policy_net, target_net, tau):\n",
    "        for target_param, policy_param in zip(target_net.parameters(), policy_net.parameters()):\n",
    "            target_param.data.copy_(tau * policy_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    def train_model(self, experiences, gamma):\n",
    "        #prioritized replay training\n",
    "        if len(experiences) > 5:\n",
    "            states, actions, rewards, next_states, terminal_runs, indices, weights = experiences\n",
    "            Q_net_targets = self.target_net(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "            Q_targets = rewards + (gamma * Q_net_targets * (1 - terminal_runs)) #0 = not terminal\n",
    "            Q_pred = self.policy_net(states).gather(1, (actions.type(torch.LongTensor).to(DEVICE)))\n",
    "            loss = F.mse_loss(Q_pred, Q_targets)\n",
    "            loss = (loss * weights)\n",
    "            prios = loss + 1e-5\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        #replay memory training\n",
    "        else:\n",
    "            states, actions, rewards, next_states, terminal_runs = experiences\n",
    "            Q_net_targets = self.target_net(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "            Q_targets = rewards + (gamma * Q_net_targets * (1 - terminal_runs))\n",
    "            Q_pred = self.policy_net(states).gather(1, (actions.type(torch.LongTensor)).to(DEVICE))\n",
    "            loss = F.mse_loss(Q_pred, Q_targets)\n",
    "        \n",
    "        #update loss and backprop\n",
    "        self.Loss = loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.soft_update(policy_net=self.policy_net, target_net=self.target_net, tau=self.tau)\n",
    "\n",
    "    def get_Loss(self):\n",
    "        return self.Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d20ac58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.seed(0)\n",
    "date = datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56fca247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.9995):\n",
    "    \n",
    "    #reporting vars\n",
    "    scores = []  # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    episode_length_window = deque(maxlen=100)\n",
    "    avg_loss_window = deque(maxlen=100)\n",
    "    avg_loss_list = []\n",
    "    episode_length_list = []\n",
    "    eps = eps_start\n",
    "    start = time.time()\n",
    "    \n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        training_episode = i_episode\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        avg_loss = 0\n",
    "        epis_length = 0\n",
    "        for t in range(max_t):\n",
    "            action = lunar_agent.decide_action(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            lunar_agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            avg_loss += float(lunar_agent.get_Loss())\n",
    "            epis_length += 1\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        #append scores\n",
    "        episode_length_list.append(epis_length)\n",
    "        episode_length_window.append(epis_length)\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        avg_loss_window.append(avg_loss)\n",
    "        avg_loss_list.append(avg_loss)\n",
    "        eps = max(eps_end, eps_decay * eps)\n",
    "        \n",
    "        #REMOVE FROM END\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f} \\taverage Loss {:.2f} \\tepisode length {:.2f}'.format(i_episode,np.mean(scores_window),\n",
    "                                                                                                         np.mean(avg_loss_window), np.mean( episode_length_window)),end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f} \\taverage Loss {:.2f} \\tepisode length {:.2f}'.format(i_episode, np.mean( scores_window), np.mean( avg_loss_window), np.mean(episode_length_window)))\n",
    "            torch.save(lunar_agent.policy_net.state_dict(), \"./results/checkpoint_random_lunar_agent_\"+date+\".pth\")\n",
    "    \n",
    "    torch.save(lunar_agent.policy_net.state_dict(), \"./results/checkpoint_final_random_lunar_agent_\"+date+\".pth\")\n",
    "    return scores, avg_loss_list, episode_length_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43abc6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -185.28 \taverage Loss 11406.55 \tepisode length 92.79\n",
      "Episode 200\tAverage Score: -203.14 \taverage Loss 9034.70 \tepisode length 95.447\n",
      "Episode 300\tAverage Score: -161.58 \taverage Loss 6798.02 \tepisode length 100.05\n",
      "Episode 400\tAverage Score: -167.89 \taverage Loss 4438.86 \tepisode length 90.170\n",
      "Episode 500\tAverage Score: -200.45 \taverage Loss 3794.30 \tepisode length 90.39\n",
      "Episode 600\tAverage Score: -193.19 \taverage Loss 3198.68 \tepisode length 91.85\n",
      "Episode 700\tAverage Score: -197.68 \taverage Loss 3277.45 \tepisode length 93.83\n",
      "Episode 800\tAverage Score: -188.05 \taverage Loss 2983.83 \tepisode length 88.28\n",
      "Episode 900\tAverage Score: -179.99 \taverage Loss 3182.87 \tepisode length 90.50\n",
      "Episode 1000\tAverage Score: -175.79 \taverage Loss 3042.07 \tepisode length 91.60\n",
      "Episode 1100\tAverage Score: -190.72 \taverage Loss 3134.78 \tepisode length 92.13\n",
      "Episode 1200\tAverage Score: -172.40 \taverage Loss 3159.85 \tepisode length 90.64\n",
      "Episode 1300\tAverage Score: -186.60 \taverage Loss 3234.62 \tepisode length 91.29\n",
      "Episode 1400\tAverage Score: -176.72 \taverage Loss 3450.60 \tepisode length 102.01\n",
      "Episode 1500\tAverage Score: -200.77 \taverage Loss 3297.06 \tepisode length 92.876\n",
      "Episode 1600\tAverage Score: -185.57 \taverage Loss 3307.69 \tepisode length 90.57\n",
      "Episode 1700\tAverage Score: -185.24 \taverage Loss 3202.88 \tepisode length 88.63\n",
      "Episode 1800\tAverage Score: -186.52 \taverage Loss 3376.28 \tepisode length 94.80\n",
      "Episode 1900\tAverage Score: -185.41 \taverage Loss 3512.44 \tepisode length 96.06\n",
      "Episode 2000\tAverage Score: -194.54 \taverage Loss 3276.80 \tepisode length 90.17\n"
     ]
    }
   ],
   "source": [
    "lunar_agent = LL(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, noisy = False, pr_xp = True)\n",
    "\n",
    "#set training episode to 0\n",
    "training_episode = 0\n",
    "\n",
    "#train agent\n",
    "scores, avg_loss_list, episode_length_list = train(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.9995)\n",
    "\n",
    "#save scores\n",
    "scores = np.array(scores)\n",
    "losslist = np.array(avg_loss_list)\n",
    "lengthlist = np.array(episode_length_list)\n",
    "df = pd.DataFrame(scores, columns=['Scores'])\n",
    "df['Loss'] = losslist\n",
    "df['Episode length'] = lengthlist\n",
    "df.to_csv(\"./results/random_res_\"+date+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
