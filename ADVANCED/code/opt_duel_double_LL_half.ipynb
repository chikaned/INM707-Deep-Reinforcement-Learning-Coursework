{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb646772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DSProjects\\Anaconda\\envs\\torch_gym\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transform\n",
    "import gym\n",
    "import warnings\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b80bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import DQN, Duel_DQN, NoisyLinear\n",
    "from memory import ReplayMemory\n",
    "from LunLand import LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d20ac58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.seed(0)\n",
    "date = datetime.now().strftime(\"%Y_%m_%d-%I_%M_%S_%p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56fca247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.9995):\n",
    "    \n",
    "    #reporting vars\n",
    "    scores = []  # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    episode_length_window = deque(maxlen=100)\n",
    "    avg_loss_window = deque(maxlen=100)\n",
    "    avg_loss_list = []\n",
    "    episode_length_list = []\n",
    "    eps = eps_start\n",
    "    start = time.time()\n",
    "    \n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        training_episode = i_episode\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        avg_loss = 0\n",
    "        epis_length = 0\n",
    "        for t in range(max_t):\n",
    "            action = lunar_agent.decide_action(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            lunar_agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            avg_loss += float(lunar_agent.get_Loss())\n",
    "            epis_length += 1\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        #append scores\n",
    "        episode_length_list.append(epis_length)\n",
    "        episode_length_window.append(epis_length)\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        avg_loss_window.append(avg_loss)\n",
    "        avg_loss_list.append(avg_loss)\n",
    "        eps = max(eps_end, eps_decay * eps)\n",
    "        \n",
    "        #REMOVE FROM END\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f} \\taverage Loss {:.2f} \\tepisode length {:.2f}'.format(i_episode,np.mean(scores_window),\n",
    "                                                                                                         np.mean(avg_loss_window), np.mean( episode_length_window)),end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f} \\taverage Loss {:.2f} \\tepisode length {:.2f}'.format(i_episode, np.mean( scores_window), np.mean( avg_loss_window), np.mean(episode_length_window)))\n",
    "            torch.save(lunar_agent.policy_net.state_dict(), \"./results/checkpoint_duel_dDQN_half_lunar_agent_\"+date+str(i_episode)+\".pth\")\n",
    "    \n",
    "    torch.save(lunar_agent.policy_net.state_dict(), \"./results/checkpoint_final_duel_dDQN_half_lunar_agent_\"+date+\".pth\")\n",
    "    return scores, avg_loss_list, episode_length_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43abc6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -160.96 \taverage Loss 6536.00 \tepisode length 193.26\n",
      "Episode 200\tAverage Score: -32.75 \taverage Loss 2607.66 \tepisode length 239.250\n",
      "Episode 300\tAverage Score: -3.79 \taverage Loss 2044.62 \tepisode length 248.109\n",
      "Episode 400\tAverage Score: 7.66 \taverage Loss 1443.06 \tepisode length 240.246\n",
      "Episode 500\tAverage Score: -1.19 \taverage Loss 1189.97 \tepisode length 236.09\n",
      "Episode 600\tAverage Score: -11.71 \taverage Loss 883.34 \tepisode length 237.09\n",
      "Episode 700\tAverage Score: 7.68 \taverage Loss 951.15 \tepisode length 240.6397\n",
      "Episode 800\tAverage Score: 23.41 \taverage Loss 878.93 \tepisode length 243.72\n"
     ]
    }
   ],
   "source": [
    "\"\"\"class LL:\n",
    "    def __init__(self, state_size, action_size, seed, batch_size=64, gamma=0.99, learning_rate=1e-4,\n",
    "                 buffer_size=int(1e5), n_every=4, tau=1e-3, device = DEVICE, noisy = False, dueling = False, dDQN = False):\"\"\"\n",
    "\n",
    "tau = 0.0007161331306967163\n",
    "learning_rate = 0.0020105330891440187\n",
    "gamma = 0.9997133549438102\n",
    "eps_start = 0.9221004582368262\n",
    "eps_end = 0.023580916333788057\n",
    "eps_decay = 0.9523200490821455\n",
    "    \n",
    "lunar_agent = LL(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, gamma = gamma,\n",
    "                 learning_rate=learning_rate, buffer_size=int(1e5), n_every = 4, tau=tau, device = DEVICE, \n",
    "                 noisy = False, dueling = True, dDQN = True)\n",
    "\n",
    "#set training episode to 0\n",
    "training_episode = 0\n",
    "\n",
    "#train agent\n",
    "scores, avg_loss_list, episode_length_list = train(n_episodes=800, max_t=500, eps_start=eps_start, eps_end=eps_end, eps_decay=eps_decay)\n",
    "\n",
    "#save scores\n",
    "scores = np.array(scores)\n",
    "losslist = np.array(avg_loss_list)\n",
    "lengthlist = np.array(episode_length_list)\n",
    "df = pd.DataFrame(scores, columns=['Scores'])\n",
    "df['Loss'] = losslist\n",
    "df['Episode length'] = lengthlist\n",
    "df.to_csv(\"./results/opt_duel_dDQN_half_res_\"+date+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
