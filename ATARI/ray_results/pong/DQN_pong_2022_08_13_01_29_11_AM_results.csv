,episode_reward_max,episode_reward_min,episode_reward_mean,episode_len_mean,episodes_this_iter,num_healthy_workers,num_agent_steps_sampled,num_agent_steps_trained,num_env_steps_sampled,num_env_steps_trained,num_env_steps_sampled_this_iter,num_env_steps_trained_this_iter,timesteps_total,agent_timesteps_total,done,episodes_total,training_iteration,trial_id,experiment_id,date,timestamp,time_this_iter_s,time_total_s,pid,hostname,node_ip,time_since_restore,timesteps_since_restore,iterations_since_restore,warmup_time,info/num_env_steps_sampled,info/num_env_steps_trained,info/num_agent_steps_sampled,info/num_agent_steps_trained,info/last_target_update_ts,info/num_target_updates,sampler_results/episode_reward_max,sampler_results/episode_reward_min,sampler_results/episode_reward_mean,sampler_results/episode_len_mean,sampler_results/episodes_this_iter,hist_stats/episode_reward,hist_stats/episode_lengths,timers/training_iteration_time_ms,timers/load_time_ms,timers/load_throughput,timers/learn_time_ms,timers/learn_throughput,timers/synch_weights_time_ms,counters/num_env_steps_sampled,counters/num_env_steps_trained,counters/num_agent_steps_sampled,counters/num_agent_steps_trained,counters/last_target_update_ts,counters/num_target_updates,perf/cpu_util_percent,perf/ram_util_percent,perf/gpu_util_percent0,perf/vram_util_percent0,sampler_results/hist_stats/episode_reward,sampler_results/hist_stats/episode_lengths,info/learner/default_policy/td_error,info/learner/default_policy/mean_td_error,info/learner/default_policy/learner_stats/mean_q,info/learner/default_policy/learner_stats/min_q,info/learner/default_policy/learner_stats/max_q,info/learner/default_policy/learner_stats/cur_lr,config/_disable_action_flattening,config/_disable_execution_plan_api,config/_disable_preprocessor_api,config/_fake_gpus,config/_tf_policy_handles_more_than_one_loss,config/action_space,config/actions_in_input_normalized,config/adam_epsilon,config/always_attach_evaluation_results,config/batch_mode,config/before_learn_on_batch,config/buffer_size,config/callbacks,config/clip_actions,config/clip_rewards,config/collect_metrics_timeout,config/compress_observations,config/create_env_on_driver,config/custom_eval_function,config/custom_resources_per_worker,config/disable_env_checking,config/double_q,config/dueling,config/eager_max_retraces,config/eager_tracing,config/env,config/env_config,config/env_task_fn,config/evaluation_config,config/evaluation_duration,config/evaluation_duration_unit,config/evaluation_interval,config/evaluation_num_episodes,config/evaluation_num_workers,config/evaluation_parallel_to_training,config/exploration_config,config/explore,config/extra_python_environs_for_driver,config/extra_python_environs_for_worker,config/fake_sampler,config/framework,config/gamma,config/grad_clip,config/hiddens,config/horizon,config/ignore_worker_failures,config/in_evaluation,config/input,config/input_config,config/input_evaluation,config/keep_per_episode_custom_metrics,config/learning_starts,config/local_tf_session_args,config/log_level,config/log_sys_usage,config/logger_config,config/lr,config/lr_schedule,config/metrics_episode_collection_timeout_s,config/metrics_num_episodes_for_smoothing,config/metrics_smoothing_episodes,config/min_iter_time_s,config/min_sample_timesteps_per_reporting,config/min_time_s_per_reporting,config/min_train_timesteps_per_reporting,config/model,config/monitor,config/multiagent,config/n_step,config/no_done_at_end,config/noisy,config/normalize_actions,config/num_atoms,config/num_cpus_for_driver,config/num_cpus_per_worker,config/num_envs_per_worker,config/num_gpus,config/num_gpus_per_worker,config/num_workers,config/observation_filter,config/observation_space,config/optimizer,config/output,config/output_compress_columns,config/output_config,config/output_max_file_size,config/placement_strategy,config/postprocess_inputs,config/preprocessor_pref,config/prioritized_replay,config/prioritized_replay_alpha,config/prioritized_replay_beta,config/prioritized_replay_eps,config/record_env,config/recreate_failed_workers,config/remote_env_batch_wait_ms,config/remote_worker_envs,config/render_env,config/replay_batch_size,config/replay_buffer_config,config/replay_sequence_length,config/rollout_fragment_length,config/sample_async,config/sample_collector,config/seed,config/shuffle_buffer_size,config/sigma0,config/simple_optimizer,config/soft_horizon,config/store_buffer_in_checkpoints,config/synchronize_filters,config/target_network_update_freq,config/tf_session_args,config/timesteps_per_iteration,config/train_batch_size,config/training_intensity,config/v_max,config/v_min,config/worker_side_prioritization,logdir
0,-3.0,-21.0,-16.93,4990.82,0,2,1110000,4436032,1110000,4436032,1000,4000,1110000,1110000,True,697,1110,5f6ab_00000,011b9a1d3765469a95f43bf66bcee258,2022-08-13_01-29-10,1660350550,9.537145137786863,10033.37274813652,10704,DESKTOP-UFRQPKH,127.0.0.1,10033.37274813652,0,1110,7.328903675079346,1110000,4436032,1110000,4436032,1109800,2201,-3.0,-21.0,-16.93,4990.82,0,"[-18.0, -20.0, -19.0, -20.0, -20.0, -20.0, -20.0, -17.0, -17.0, -21.0, -21.0, -18.0, -21.0, -20.0, -19.0, -16.0, -21.0, -20.0, -19.0, -18.0, -21.0, -19.0, -16.0, -21.0, -20.0, -18.0, -21.0, -21.0, -17.0, -20.0, -20.0, -19.0, -19.0, -20.0, -19.0, -20.0, -19.0, -16.0, -18.0, -20.0, -19.0, -19.0, -15.0, -20.0, -18.0, -21.0, -20.0, -18.0, -18.0, -18.0, -20.0, -14.0, -15.0, -16.0, -21.0, -18.0, -18.0, -21.0, -21.0, -19.0, -14.0, -20.0, -20.0, -17.0, -16.0, -15.0, -18.0, -18.0, -17.0, -14.0, -14.0, -16.0, -20.0, -18.0, -13.0, -12.0, -14.0, -11.0, -12.0, -12.0, -12.0, -9.0, -15.0, -12.0, -5.0, -14.0, -14.0, -17.0, -12.0, -10.0, -16.0, -15.0, -8.0, -7.0, -16.0, -12.0, -17.0, -10.0, -20.0, -3.0]","[1873, 1452, 2190, 2238, 2368, 1678, 3092, 4041, 4109, 3839, 4284, 2134, 2379, 3187, 5857, 4336, 3285, 3234, 10666, 2099, 5008, 12173, 8557, 2826, 10043, 2653, 3901, 2566, 2550, 5878, 8996, 6256, 2359, 12093, 4593, 3248, 4027, 5228, 3998, 2336, 2767, 5259, 4824, 7242, 4180, 2159, 1154, 4420, 3535, 4495, 5880, 4139, 4505, 6549, 12715, 3448, 2879, 1993, 2005, 3321, 6349, 4130, 8997, 6430, 10152, 6096, 3218, 4427, 3983, 5346, 7401, 3759, 6802, 3948, 5915, 8055, 5713, 4246, 5378, 4262, 6636, 6558, 6860, 6301, 8707, 7208, 8677, 1941, 7094, 2926, 2947, 4018, 4597, 6080, 5332, 5549, 7979, 11878, 2160, 5928]",83.619,2.001,15994.867,33.007,969.479,6.802,1110000,4436032,1110000,4436032,1109800,2201,18.666666666666668,41.00000000000001,0.3033333333333333,0.2728271484375,"[-18.0, -20.0, -19.0, -20.0, -20.0, -20.0, -20.0, -17.0, -17.0, -21.0, -21.0, -18.0, -21.0, -20.0, -19.0, -16.0, -21.0, -20.0, -19.0, -18.0, -21.0, -19.0, -16.0, -21.0, -20.0, -18.0, -21.0, -21.0, -17.0, -20.0, -20.0, -19.0, -19.0, -20.0, -19.0, -20.0, -19.0, -16.0, -18.0, -20.0, -19.0, -19.0, -15.0, -20.0, -18.0, -21.0, -20.0, -18.0, -18.0, -18.0, -20.0, -14.0, -15.0, -16.0, -21.0, -18.0, -18.0, -21.0, -21.0, -19.0, -14.0, -20.0, -20.0, -17.0, -16.0, -15.0, -18.0, -18.0, -17.0, -14.0, -14.0, -16.0, -20.0, -18.0, -13.0, -12.0, -14.0, -11.0, -12.0, -12.0, -12.0, -9.0, -15.0, -12.0, -5.0, -14.0, -14.0, -17.0, -12.0, -10.0, -16.0, -15.0, -8.0, -7.0, -16.0, -12.0, -17.0, -10.0, -20.0, -3.0]","[1873, 1452, 2190, 2238, 2368, 1678, 3092, 4041, 4109, 3839, 4284, 2134, 2379, 3187, 5857, 4336, 3285, 3234, 10666, 2099, 5008, 12173, 8557, 2826, 10043, 2653, 3901, 2566, 2550, 5878, 8996, 6256, 2359, 12093, 4593, 3248, 4027, 5228, 3998, 2336, 2767, 5259, 4824, 7242, 4180, 2159, 1154, 4420, 3535, 4495, 5880, 4139, 4505, 6549, 12715, 3448, 2879, 1993, 2005, 3321, 6349, 4130, 8997, 6430, 10152, 6096, 3218, 4427, 3983, 5346, 7401, 3759, 6802, 3948, 5915, 8055, 5713, 4246, 5378, 4262, 6636, 6558, 6860, 6301, 8707, 7208, 8677, 1941, 7094, 2926, 2947, 4018, 4597, 6080, 5332, 5549, 7979, 11878, 2160, 5928]","[ 0.02662656  0.00822394  0.83501387  0.01393265 -0.01702294 -0.06506041
  0.0158882  -0.11616896 -0.01626411 -0.08113825  0.02431949 -0.06838498
 -0.07472289 -0.01701017  0.24629289  0.01877721 -0.02644517 -0.010737
  0.0413359   0.02431949 -0.02625992 -0.02559696 -0.01631498  0.00971221
 -0.09237546 -0.0643096   0.02423748 -0.05494903  0.01859049 -0.0011016
  0.06928974  0.24772525]",0.0265757143497467,0.0949445217847824,-0.777294397354126,0.316991925239563,0.0001,False,True,False,False,False,,False,1e-08,False,truncate_episodes,,-1,<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,False,,-1,True,False,,{},False,True,True,20,False,PongDeterministic-v4,{},,{'explore': False},10,episodes,,-1,0,False,"{'epsilon_timesteps': 200000, 'final_epsilon': 0.01, 'initial_epsilon': 1.0, 'type': 'EpsilonGreedy'}",True,{},{},False,torch,0.99,40,[256],,False,False,sampler,{},"['is', 'wis']",False,-1,"{'inter_op_parallelism_threads': 8, 'intra_op_parallelism_threads': 8}",WARN,True,,0.0001,,180,100,-1,-1,,1,,"{'_disable_action_flattening': False, '_disable_preprocessor_api': False, '_time_major': False, '_use_default_native_models': False, 'attention_dim': 64, 'attention_head_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_num_heads': 1, 'attention_num_transformer_units': 1, 'attention_position_wise_mlp_dim': 32, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'conv_activation': 'relu', 'conv_filters': None, 'custom_action_dist': None, 'custom_model': None, 'custom_model_config': {}, 'custom_preprocessor': None, 'dim': 42, 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'framestack': True, 'free_log_std': False, 'grayscale': True, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_action_reward': -1, 'lstm_use_prev_reward': False, 'max_seq_len': 20, 'no_final_linear': False, 'post_fcnet_activation': 'relu', 'post_fcnet_hiddens': [], 'use_attention': False, 'use_lstm': False, 'vf_share_layers': True, 'zero_mean': True}",-1,"{'count_steps_by': 'env_steps', 'observation_fn': None, 'policies': {}, 'policies_to_train': None, 'policy_map_cache': None, 'policy_map_capacity': 100, 'policy_mapping_fn': None, 'replay_mode': 'independent'}",1,False,False,True,1,1,2,1,0.25,0,2,NoFilter,,{},,"['obs', 'new_obs']",{},67108864,PACK,False,deepmind,-1,-1,-1,-1,False,False,0,False,False,-1,"{'_enable_replay_buffer_api': True, 'capacity': 50000, 'prioritized_replay_alpha': 0.6, 'prioritized_replay_beta': 0.4, 'prioritized_replay_eps': 1e-06, 'replay_sequence_length': 1, 'type': 'MultiAgentPrioritizedReplayBuffer'}",-1,4,False,<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,,0,0.5,-1,False,False,True,500,"{'allow_soft_placement': True, 'device_count': {'CPU': 1}, 'gpu_options': {'allow_growth': True}, 'inter_op_parallelism_threads': 2, 'intra_op_parallelism_threads': 2, 'log_device_placement': False}",1000,32,,10.0,-10.0,False,C:\Users\josep\Desktop\data_science_MSc\Term 2\INM707 Deep Reinforcement Learning\project\work_dir\ATARI\pong_dqn\pong_dqn\DQN_PongDeterministic-v4_5f6ab_00000_0_2022-08-12_22-40-24
