,episode_reward_max,episode_reward_min,episode_reward_mean,episode_len_mean,episodes_this_iter,num_healthy_workers,num_agent_steps_sampled,num_agent_steps_trained,num_env_steps_sampled,num_env_steps_trained,num_env_steps_sampled_this_iter,num_env_steps_trained_this_iter,timesteps_total,agent_timesteps_total,done,episodes_total,training_iteration,trial_id,experiment_id,date,timestamp,time_this_iter_s,time_total_s,pid,hostname,node_ip,time_since_restore,timesteps_since_restore,iterations_since_restore,warmup_time,info/num_env_steps_sampled,info/num_env_steps_trained,info/num_agent_steps_sampled,info/num_agent_steps_trained,sampler_results/episode_reward_max,sampler_results/episode_reward_min,sampler_results/episode_reward_mean,sampler_results/episode_len_mean,sampler_results/episodes_this_iter,hist_stats/episode_reward,hist_stats/episode_lengths,sampler_perf/mean_raw_obs_processing_ms,sampler_perf/mean_inference_ms,sampler_perf/mean_action_processing_ms,sampler_perf/mean_env_wait_ms,sampler_perf/mean_env_render_ms,timers/training_iteration_time_ms,timers/load_time_ms,timers/load_throughput,timers/learn_time_ms,timers/learn_throughput,timers/update_time_ms,counters/num_env_steps_sampled,counters/num_env_steps_trained,counters/num_agent_steps_sampled,counters/num_agent_steps_trained,perf/cpu_util_percent,perf/ram_util_percent,perf/gpu_util_percent0,perf/vram_util_percent0,sampler_results/hist_stats/episode_reward,sampler_results/hist_stats/episode_lengths,sampler_results/sampler_perf/mean_raw_obs_processing_ms,sampler_results/sampler_perf/mean_inference_ms,sampler_results/sampler_perf/mean_action_processing_ms,sampler_results/sampler_perf/mean_env_wait_ms,sampler_results/sampler_perf/mean_env_render_ms,info/learner/default_policy/learner_stats/cur_kl_coeff,info/learner/default_policy/learner_stats/cur_lr,info/learner/default_policy/learner_stats/total_loss,info/learner/default_policy/learner_stats/policy_loss,info/learner/default_policy/learner_stats/vf_loss,info/learner/default_policy/learner_stats/vf_explained_var,info/learner/default_policy/learner_stats/kl,info/learner/default_policy/learner_stats/entropy,info/learner/default_policy/learner_stats/entropy_coeff,config/_disable_action_flattening,config/_disable_execution_plan_api,config/_disable_preprocessor_api,config/_fake_gpus,config/_tf_policy_handles_more_than_one_loss,config/action_space,config/actions_in_input_normalized,config/always_attach_evaluation_results,config/batch_mode,config/callbacks,config/clip_actions,config/clip_param,config/clip_rewards,config/collect_metrics_timeout,config/compress_observations,config/count_steps_by,config/create_env_on_driver,config/custom_eval_function,config/custom_resources_per_worker,config/disable_env_checking,config/eager_max_retraces,config/eager_tracing,config/entropy_coeff,config/entropy_coeff_schedule,config/env,config/env_config,config/env_task_fn,config/evaluation_config,config/evaluation_duration,config/evaluation_duration_unit,config/evaluation_interval,config/evaluation_num_episodes,config/evaluation_num_workers,config/evaluation_parallel_to_training,config/exploration_config,config/explore,config/extra_python_environs_for_driver,config/extra_python_environs_for_worker,config/fake_sampler,config/framework,config/gamma,config/grad_clip,config/horizon,config/ignore_worker_failures,config/in_evaluation,config/input,config/input_config,config/input_evaluation,config/keep_per_episode_custom_metrics,config/kl_coeff,config/kl_target,config/lambda,config/local_tf_session_args,config/log_level,config/log_sys_usage,config/logger_config,config/lr,config/lr_schedule,config/metrics_episode_collection_timeout_s,config/metrics_num_episodes_for_smoothing,config/metrics_smoothing_episodes,config/min_iter_time_s,config/min_sample_timesteps_per_reporting,config/min_time_s_per_reporting,config/min_train_timesteps_per_reporting,config/model,config/monitor,config/multiagent,config/no_done_at_end,config/normalize_actions,config/num_cpus_for_driver,config/num_cpus_per_worker,config/num_envs_per_worker,config/num_gpus,config/num_gpus_per_worker,config/num_sgd_iter,config/num_workers,config/observation_filter,config/observation_fn,config/observation_space,config/optimizer,config/output,config/output_compress_columns,config/output_config,config/output_max_file_size,config/placement_strategy,config/policies,config/policies_to_train,config/policy_map_cache,config/policy_map_capacity,config/policy_mapping_fn,config/postprocess_inputs,config/preprocessor_pref,config/record_env,config/recreate_failed_workers,config/remote_env_batch_wait_ms,config/remote_worker_envs,config/render_env,config/replay_mode,config/rollout_fragment_length,config/sample_async,config/sample_collector,config/seed,config/sgd_minibatch_size,config/shuffle_buffer_size,config/shuffle_sequences,config/simple_optimizer,config/soft_horizon,config/synchronize_filters,config/tf_session_args,config/timesteps_per_iteration,config/train_batch_size,config/use_critic,config/use_gae,config/vf_clip_param,config/vf_loss_coeff,logdir
0,12.0,-18.0,-6.28,2659.87,3,2,1110000,1110000,1110000,1110000,5000,5000,1110000,1110000,True,709,222,4f95c_00000,cc32e57f71a74999a95fa13e82a103ae,2022-08-13_12-22-09,1660389729,51.59658360481262,11941.133100032806,9352,DESKTOP-UFRQPKH,127.0.0.1,11941.133100032806,0,222,7.067194223403931,1110000,1110000,1110000,1110000,12.0,-18.0,-6.28,2659.87,3,"[-17.0, -10.0, -11.0, -11.0, -12.0, -10.0, -12.0, -17.0, -18.0, -6.0, -17.0, -14.0, -17.0, -9.0, -10.0, -13.0, -18.0, -13.0, -8.0, -12.0, -4.0, -10.0, -12.0, -11.0, -7.0, -14.0, -13.0, -14.0, -12.0, -6.0, -7.0, -7.0, -14.0, -6.0, -12.0, -12.0, -11.0, -8.0, -6.0, -11.0, -14.0, -9.0, -12.0, -8.0, -12.0, -7.0, -1.0, -10.0, -11.0, -5.0, -11.0, 3.0, 1.0, -12.0, -12.0, -6.0, -4.0, -4.0, -2.0, -5.0, -6.0, -4.0, -5.0, -4.0, -6.0, -8.0, -13.0, -5.0, -12.0, -4.0, -4.0, -1.0, -2.0, -7.0, -6.0, -2.0, -5.0, -3.0, 12.0, -3.0, 11.0, -3.0, 4.0, -2.0, -8.0, 4.0, 5.0, -1.0, 3.0, -4.0, -8.0, 1.0, 9.0, -1.0, 4.0, 7.0, 9.0, 8.0, 9.0, 6.0]","[2309, 2989, 2494, 2555, 2562, 2592, 2376, 2221, 2000, 3147, 2279, 2507, 2308, 2593, 2671, 2733, 2005, 2327, 2557, 2422, 3012, 2545, 2500, 2447, 2702, 1726, 2392, 2133, 2210, 2643, 2563, 2889, 2293, 2775, 2306, 2176, 2600, 2576, 2662, 2519, 2287, 2730, 2167, 2563, 2345, 2587, 3136, 2404, 2490, 3065, 2254, 2942, 3111, 2194, 2491, 2780, 3125, 2871, 2897, 3014, 2817, 2923, 2790, 3012, 2813, 2511, 2313, 2995, 2121, 2917, 2873, 3323, 3147, 2746, 2841, 3027, 3038, 3183, 2473, 2905, 2466, 3075, 3050, 2910, 2850, 2754, 2671, 3381, 3057, 3159, 2495, 3233, 2445, 3044, 2921, 2715, 2384, 2662, 2364, 2814]",0.3618960725120626,5.117257616197984,0.0570101361236994,1.5612482338232003,0.0,51433.169,101.622,49201.721,33193.59,150.631,7.602,1110000,1110000,1110000,1110000,12.295384615384616,48.39692307692308,0.5616923076923077,0.3516845703125,"[-17.0, -10.0, -11.0, -11.0, -12.0, -10.0, -12.0, -17.0, -18.0, -6.0, -17.0, -14.0, -17.0, -9.0, -10.0, -13.0, -18.0, -13.0, -8.0, -12.0, -4.0, -10.0, -12.0, -11.0, -7.0, -14.0, -13.0, -14.0, -12.0, -6.0, -7.0, -7.0, -14.0, -6.0, -12.0, -12.0, -11.0, -8.0, -6.0, -11.0, -14.0, -9.0, -12.0, -8.0, -12.0, -7.0, -1.0, -10.0, -11.0, -5.0, -11.0, 3.0, 1.0, -12.0, -12.0, -6.0, -4.0, -4.0, -2.0, -5.0, -6.0, -4.0, -5.0, -4.0, -6.0, -8.0, -13.0, -5.0, -12.0, -4.0, -4.0, -1.0, -2.0, -7.0, -6.0, -2.0, -5.0, -3.0, 12.0, -3.0, 11.0, -3.0, 4.0, -2.0, -8.0, 4.0, 5.0, -1.0, 3.0, -4.0, -8.0, 1.0, 9.0, -1.0, 4.0, 7.0, 9.0, 8.0, 9.0, 6.0]","[2309, 2989, 2494, 2555, 2562, 2592, 2376, 2221, 2000, 3147, 2279, 2507, 2308, 2593, 2671, 2733, 2005, 2327, 2557, 2422, 3012, 2545, 2500, 2447, 2702, 1726, 2392, 2133, 2210, 2643, 2563, 2889, 2293, 2775, 2306, 2176, 2600, 2576, 2662, 2519, 2287, 2730, 2167, 2563, 2345, 2587, 3136, 2404, 2490, 3065, 2254, 2942, 3111, 2194, 2491, 2780, 3125, 2871, 2897, 3014, 2817, 2923, 2790, 3012, 2813, 2511, 2313, 2995, 2121, 2917, 2873, 3323, 3147, 2746, 2841, 3027, 3038, 3183, 2473, 2905, 2466, 3075, 3050, 2910, 2850, 2754, 2671, 3381, 3057, 3159, 2495, 3233, 2445, 3044, 2921, 2715, 2384, 2662, 2364, 2814]",0.3618960725120626,5.117257616197984,0.0570101361236994,1.5612482338232003,0.0,1.4836824602749686e-67,5.000000000000001e-05,0.0179776723393135,-0.0128875407668897,0.0416652660661687,0.7580240960813995,0.0028666093609191,1.0800052971921414,0.01,False,True,False,False,False,,False,False,truncate_episodes,<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,False,0.1,True,-1,False,env_steps,False,,{},False,20,False,0.01,,PongDeterministic-v4,{},,{},10,episodes,0,-1,0,False,{'type': 'StochasticSampling'},True,{},{},False,torch,0.99,,,False,False,sampler,{},"['is', 'wis']",False,0.5,0.01,0.95,"{'inter_op_parallelism_threads': 8, 'intra_op_parallelism_threads': 8}",WARN,True,,5e-05,,180,100,-1,-1,,,,"{'_disable_action_flattening': False, '_disable_preprocessor_api': False, '_time_major': False, '_use_default_native_models': False, 'attention_dim': 64, 'attention_head_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_num_heads': 1, 'attention_num_transformer_units': 1, 'attention_position_wise_mlp_dim': 32, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'conv_activation': 'relu', 'conv_filters': None, 'custom_action_dist': None, 'custom_model': None, 'custom_model_config': {}, 'custom_preprocessor': None, 'dim': 42, 'fcnet_activation': 'tanh', 'fcnet_hiddens': [256, 256], 'framestack': True, 'free_log_std': False, 'grayscale': False, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_action_reward': -1, 'lstm_use_prev_reward': False, 'max_seq_len': 20, 'no_final_linear': False, 'post_fcnet_activation': 'relu', 'post_fcnet_hiddens': [], 'use_attention': False, 'use_lstm': False, 'vf_share_layers': False, 'zero_mean': True}",-1,"{'count_steps_by': 'env_steps', 'observation_fn': None, 'policies': {}, 'policies_to_train': None, 'policy_map_cache': None, 'policy_map_capacity': 100, 'policy_mapping_fn': None, 'replay_mode': 'independent'}",False,True,1,3,1,0.75,0,30,2,NoFilter,,,{},,"['obs', 'new_obs']",{},67108864,PACK,{},,,100,,False,deepmind,False,False,0,False,False,independent,20,False,<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,,128,0,True,-1,False,True,"{'allow_soft_placement': True, 'device_count': {'CPU': 1}, 'gpu_options': {'allow_growth': True}, 'inter_op_parallelism_threads': 2, 'intra_op_parallelism_threads': 2, 'log_device_placement': False}",0,5000,True,True,10.0,1.0,C:\Users\josep\Desktop\data_science_MSc\Term 2\INM707 Deep Reinforcement Learning\project\work_dir\ATARI\breakout_ppo\PPOTrainer_2022-08-13_09-02-43\PPOTrainer_PongDeterministic-v4_4f95c_00000_0_2022-08-13_09-02-44
